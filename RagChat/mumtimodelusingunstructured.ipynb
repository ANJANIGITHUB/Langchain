{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "431c70f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from unstructured.partition.pdf import partition_pdf\n",
    "#import pytesseract\n",
    "import uuid\n",
    "\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.retrievers.multi_vector import MultiVectorRetriever\n",
    "from langchain.schema.document import Document\n",
    "from langchain.storage import InMemoryStore\n",
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "import base64\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.schema.messages import HumanMessage, AIMessage\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "f4ad47dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: No languages specified, defaulting to English.\n"
     ]
    }
   ],
   "source": [
    "input_path = os.getcwd()\n",
    "output_path = os.path.join(os.getcwd(), \"figures\")\n",
    "\n",
    "raw_pdf_elements = partition_pdf(\n",
    "    filename=os.path.join(input_path, \"attention_is_all_you_need.pdf\"),\n",
    "    extract_images_in_pdf=True,\n",
    "    infer_table_structure=True,\n",
    "    chunking_strategy=\"by_title\",\n",
    "    max_characters=500,\n",
    "    new_after_n_chars=400,\n",
    "    combine_text_under_n_chars=490,\n",
    "    image_output_dir_path=output_path,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97b026b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f7a5763",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "c91ac3b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<unstructured.documents.elements.CompositeElement at 0x1dbc4b2a330>,\n",
       " <unstructured.documents.elements.CompositeElement at 0x1dbd605f590>,\n",
       " <unstructured.documents.elements.CompositeElement at 0x1dc19258ce0>,\n",
       " <unstructured.documents.elements.CompositeElement at 0x1dc1e8c2f30>,\n",
       " <unstructured.documents.elements.CompositeElement at 0x1dc00deea50>,\n",
       " <unstructured.documents.elements.CompositeElement at 0x1dc01735400>,\n",
       " <unstructured.documents.elements.CompositeElement at 0x1dbd591d040>,\n",
       " <unstructured.documents.elements.CompositeElement at 0x1dc017560c0>,\n",
       " <unstructured.documents.elements.CompositeElement at 0x1dc01610230>,\n",
       " <unstructured.documents.elements.CompositeElement at 0x1dbd6b63b90>,\n",
       " <unstructured.documents.elements.CompositeElement at 0x1dc00db9d30>,\n",
       " <unstructured.documents.elements.CompositeElement at 0x1dc010dede0>,\n",
       " <unstructured.documents.elements.CompositeElement at 0x1dbd2757590>,\n",
       " <unstructured.documents.elements.CompositeElement at 0x1dbc485c1a0>,\n",
       " <unstructured.documents.elements.CompositeElement at 0x1dbc485c650>,\n",
       " <unstructured.documents.elements.CompositeElement at 0x1dbc485c260>,\n",
       " <unstructured.documents.elements.CompositeElement at 0x1dbc49ec170>,\n",
       " <unstructured.documents.elements.CompositeElement at 0x1dbe24e5760>,\n",
       " <unstructured.documents.elements.CompositeElement at 0x1dbdc2379b0>,\n",
       " <unstructured.documents.elements.CompositeElement at 0x1dbdc237e90>,\n",
       " <unstructured.documents.elements.CompositeElement at 0x1dc01367410>,\n",
       " <unstructured.documents.elements.CompositeElement at 0x1dbd58cda90>,\n",
       " <unstructured.documents.elements.CompositeElement at 0x1dbd58cf050>,\n",
       " <unstructured.documents.elements.CompositeElement at 0x1dbc4a7d760>,\n",
       " <unstructured.documents.elements.CompositeElement at 0x1dbc4a7f800>,\n",
       " <unstructured.documents.elements.CompositeElement at 0x1dbc4a7e330>,\n",
       " <unstructured.documents.elements.CompositeElement at 0x1dbc4a7caa0>,\n",
       " <unstructured.documents.elements.CompositeElement at 0x1dbd6302810>,\n",
       " <unstructured.documents.elements.CompositeElement at 0x1dbc4b17b00>,\n",
       " <unstructured.documents.elements.CompositeElement at 0x1dbc4b176e0>,\n",
       " <unstructured.documents.elements.CompositeElement at 0x1dc00d813d0>,\n",
       " <unstructured.documents.elements.CompositeElement at 0x1dc01460d70>,\n",
       " <unstructured.documents.elements.CompositeElement at 0x1dbd0f3b740>,\n",
       " <unstructured.documents.elements.CompositeElement at 0x1dbd0f3ab10>,\n",
       " <unstructured.documents.elements.CompositeElement at 0x1dbd316adb0>,\n",
       " <unstructured.documents.elements.CompositeElement at 0x1dbd316b020>,\n",
       " <unstructured.documents.elements.CompositeElement at 0x1dbd3149730>,\n",
       " <unstructured.documents.elements.CompositeElement at 0x1dbd314a1b0>,\n",
       " <unstructured.documents.elements.CompositeElement at 0x1dbd314baa0>,\n",
       " <unstructured.documents.elements.CompositeElement at 0x1dc00f28080>,\n",
       " <unstructured.documents.elements.CompositeElement at 0x1dc00f293d0>,\n",
       " <unstructured.documents.elements.CompositeElement at 0x1dbc4a748c0>,\n",
       " <unstructured.documents.elements.CompositeElement at 0x1dbc4a74920>,\n",
       " <unstructured.documents.elements.CompositeElement at 0x1dc012d5a30>,\n",
       " <unstructured.documents.elements.CompositeElement at 0x1dbc45c08f0>,\n",
       " <unstructured.documents.elements.CompositeElement at 0x1dc1953e480>,\n",
       " <unstructured.documents.elements.CompositeElement at 0x1dc1953f5c0>,\n",
       " <unstructured.documents.elements.CompositeElement at 0x1dc1953ca10>,\n",
       " <unstructured.documents.elements.CompositeElement at 0x1dc0dd0fb90>,\n",
       " <unstructured.documents.elements.CompositeElement at 0x1dc0dd0fb30>,\n",
       " <unstructured.documents.elements.CompositeElement at 0x1dc0dd0fe00>,\n",
       " <unstructured.documents.elements.CompositeElement at 0x1dc0dd0ff50>,\n",
       " <unstructured.documents.elements.CompositeElement at 0x1dc0dd0fc20>,\n",
       " <unstructured.documents.elements.CompositeElement at 0x1dc0dd0ff20>,\n",
       " <unstructured.documents.elements.CompositeElement at 0x1dc0dd0eb70>,\n",
       " <unstructured.documents.elements.CompositeElement at 0x1dc0dd0f770>,\n",
       " <unstructured.documents.elements.CompositeElement at 0x1dc01114da0>,\n",
       " <unstructured.documents.elements.CompositeElement at 0x1dc01114980>,\n",
       " <unstructured.documents.elements.CompositeElement at 0x1dc01114b90>,\n",
       " <unstructured.documents.elements.CompositeElement at 0x1dc01114b60>,\n",
       " <unstructured.documents.elements.CompositeElement at 0x1dc01114740>,\n",
       " <unstructured.documents.elements.CompositeElement at 0x1dbc49b14f0>,\n",
       " <unstructured.documents.elements.CompositeElement at 0x1dbc49b1730>,\n",
       " <unstructured.documents.elements.TableChunk at 0x1dbda09cd40>,\n",
       " <unstructured.documents.elements.CompositeElement at 0x1dc1e8c38f0>,\n",
       " <unstructured.documents.elements.CompositeElement at 0x1dbc49c5b50>,\n",
       " <unstructured.documents.elements.CompositeElement at 0x1dbc49c65a0>,\n",
       " <unstructured.documents.elements.CompositeElement at 0x1dbc49a9d60>,\n",
       " <unstructured.documents.elements.CompositeElement at 0x1dbc49aa030>,\n",
       " <unstructured.documents.elements.CompositeElement at 0x1dbc49ab530>,\n",
       " <unstructured.documents.elements.CompositeElement at 0x1dbc49aa900>,\n",
       " <unstructured.documents.elements.CompositeElement at 0x1dbc49a94c0>,\n",
       " <unstructured.documents.elements.CompositeElement at 0x1dbc49aba70>,\n",
       " <unstructured.documents.elements.TableChunk at 0x1dbc47f3800>,\n",
       " <unstructured.documents.elements.TableChunk at 0x1dc00b7e570>,\n",
       " <unstructured.documents.elements.CompositeElement at 0x1dbc4a34d70>,\n",
       " <unstructured.documents.elements.CompositeElement at 0x1dbc4b0e960>,\n",
       " <unstructured.documents.elements.CompositeElement at 0x1dc18e4db80>,\n",
       " <unstructured.documents.elements.CompositeElement at 0x1dc18e4f0e0>,\n",
       " <unstructured.documents.elements.CompositeElement at 0x1dc18e4f350>,\n",
       " <unstructured.documents.elements.CompositeElement at 0x1dc013a1070>,\n",
       " <unstructured.documents.elements.TableChunk at 0x1dc1e847110>,\n",
       " <unstructured.documents.elements.TableChunk at 0x1dbd5945bb0>,\n",
       " <unstructured.documents.elements.CompositeElement at 0x1dbd5945370>,\n",
       " <unstructured.documents.elements.CompositeElement at 0x1dc013a2120>,\n",
       " <unstructured.documents.elements.CompositeElement at 0x1dc1e845d90>,\n",
       " <unstructured.documents.elements.CompositeElement at 0x1dc195762a0>,\n",
       " <unstructured.documents.elements.CompositeElement at 0x1dc19574440>,\n",
       " <unstructured.documents.elements.CompositeElement at 0x1dc19575d00>,\n",
       " <unstructured.documents.elements.CompositeElement at 0x1dc19577830>,\n",
       " <unstructured.documents.elements.CompositeElement at 0x1dc195768d0>,\n",
       " <unstructured.documents.elements.CompositeElement at 0x1dc19577170>,\n",
       " <unstructured.documents.elements.CompositeElement at 0x1dc195767e0>,\n",
       " <unstructured.documents.elements.CompositeElement at 0x1dc19576870>,\n",
       " <unstructured.documents.elements.CompositeElement at 0x1dc195750d0>,\n",
       " <unstructured.documents.elements.CompositeElement at 0x1dc19576cc0>,\n",
       " <unstructured.documents.elements.CompositeElement at 0x1dc195746e0>,\n",
       " <unstructured.documents.elements.CompositeElement at 0x1dc19574890>,\n",
       " <unstructured.documents.elements.CompositeElement at 0x1dc19577ad0>,\n",
       " <unstructured.documents.elements.CompositeElement at 0x1dc19577b30>,\n",
       " <unstructured.documents.elements.CompositeElement at 0x1dc195766c0>,\n",
       " <unstructured.documents.elements.CompositeElement at 0x1dc195769c0>,\n",
       " <unstructured.documents.elements.CompositeElement at 0x1dc195772c0>,\n",
       " <unstructured.documents.elements.CompositeElement at 0x1dc195773b0>,\n",
       " <unstructured.documents.elements.CompositeElement at 0x1dc19576630>,\n",
       " <unstructured.documents.elements.CompositeElement at 0x1dc195763c0>,\n",
       " <unstructured.documents.elements.CompositeElement at 0x1dc195775f0>,\n",
       " <unstructured.documents.elements.CompositeElement at 0x1dc19575f70>,\n",
       " <unstructured.documents.elements.CompositeElement at 0x1dbc4786b40>,\n",
       " <unstructured.documents.elements.CompositeElement at 0x1dbc47870e0>,\n",
       " <unstructured.documents.elements.CompositeElement at 0x1dbd58f4860>]"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_pdf_elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d5eaac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "for element in raw_pdf_elements:\n",
    "    print(element.metadata.to_dict())  # Access metadata for each element"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e11307d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The length of table elements are : 5\n",
      "The length of text elements are : 106\n",
      "Folder has files, proceeding...\n",
      "The length of image elements are : 7\n"
     ]
    }
   ],
   "source": [
    "import base64\n",
    "\n",
    "text_elements = []\n",
    "table_elements = []\n",
    "image_elements = []\n",
    "\n",
    "def encode_image(image_path):\n",
    "    with open(image_path, \"rb\") as image_file:\n",
    "        return base64.b64encode(image_file.read()).decode('utf-8')\n",
    "\n",
    "for element in raw_pdf_elements:\n",
    "    if 'CompositeElement' in str(type(element)):\n",
    "        text_elements.append(element)\n",
    "    elif 'Table' in str(type(element)):\n",
    "        table_elements.append(element)\n",
    "\n",
    "table_elements = [i.text for i in table_elements]\n",
    "text_elements = [i.text for i in text_elements]\n",
    "\n",
    "# Tables\n",
    "print(\"The length of table elements are :\", len(table_elements))\n",
    "\n",
    "# Text\n",
    "print(\"The length of text elements are :\", len(text_elements))\n",
    "\n",
    "if os.listdir(output_path):  # True if folder has any files or subfolders\n",
    "    print(\"Folder has files, proceeding...\")\n",
    "    for image_file in os.listdir(output_path):\n",
    "        if image_file.endswith(('.png', '.jpg', '.jpeg')):\n",
    "            image_path = os.path.join(output_path, image_file)\n",
    "            encoded_image = encode_image(image_path)\n",
    "            image_elements.append(encoded_image)\n",
    "        \n",
    "    # image\n",
    "    print(\"The length of image elements are :\",len(image_elements))\n",
    "    # ➡ Your next code block here\n",
    "else:\n",
    "    print(\"❌ Folder is empty. Skipping execution.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "805226b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain_gpt= ChatOpenAI(model=\"gpt-4o\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "baeba036",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_text(text_element):\n",
    "    prompt = f\"Summarize the following text:\\n\\n{text_element}\\n\\nSummary:\"\n",
    "    response = chain_gpt.invoke([HumanMessage(content=prompt)])\n",
    "    return response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "e7203eba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Google grants permission to reproduce the tables and figures from the paper for journalistic or scholarly purposes, provided proper attribution is given.',\n",
       " '\"Attention Is All You Need\" is a seminal paper that introduces the Transformer model, a novel architecture for sequence-to-sequence tasks. The authors, a team of researchers from Google Brain and other institutions, propose a model that relies entirely on self-attention mechanisms, dispensing with recurrent and convolutional layers traditionally used for these tasks. The Transformer is designed to enhance parallelization and improve computational efficiency. It demonstrates superior performance in tasks such as translation, setting new benchmarks and significantly reducing training time compared to previous models. The paper\\'s contributions have had a profound impact on natural language processing and have paved the way for advancements in machine learning models.',\n",
       " 'The dominant sequence transduction models typically use complex recurrent or convolutional neural networks with an encoder-decoder structure, often enhanced by an attention mechanism. This text introduces a new architecture, the Transformer, which relies solely on attention mechanisms, eliminating the need for recurrence and convolutions. Experiments on machine translation tasks demonstrate that Transformers offer superior quality.',\n",
       " 'Our model significantly improves translation tasks by achieving a BLEU score of 28.4 on the WMT 2014 English-to-German task, outperforming previous best results by over 2 BLEU points. For the English-to-French task, it sets a new single-model state-of-the-art BLEU score of 41.8, achieved in just 3.5 days using eight GPUs, which is much more efficient in terms of training time and resources compared to the best models previously reported.',\n",
       " 'Transformers are effective in English constituency parsing, performing well with both large and limited training data.',\n",
       " \"Jakob proposed the idea of replacing RNNs with self-attention and initiated its evaluation. Ashish and Illia designed and implemented the first Transformer models, playing a crucial role in the project's development. Noam introduced concepts like scaled dot-product attention, multi-head attention, and parameter-free position representation, contributing extensively to the project. Niki was responsible for designing, implementing, tuning, and evaluating the models. All contributors made equal contributions, with listing order being random.\",\n",
       " 'Llion developed novel model variants and was key in creating the initial codebase, focusing on efficient inference and visualizations. Lukasz and Aidan dedicated extensive time to designing and implementing tensor2tensor, which replaced the original codebase, significantly enhancing results and accelerating research.',\n",
       " 'The text indicates that the work referenced was conducted by individuals while they were affiliated with Google Brain and Google Research. It was presented at the 31st Conference on Neural Information Processing Systems (NIPS 2017) held in Long Beach, CA, USA.',\n",
       " 'Recurrent neural networks, particularly long short-term memory and gated recurrent networks, are recognized as leading methods for sequence modeling and transduction tasks like language modeling and machine translation. Continued research has aimed to advance recurrent language models and encoder-decoder architectures.',\n",
       " 'Recurrent models process input and output sequences by generating hidden states sequentially, with each state dependent on the previous one and current input. This sequential processing hinders parallelization within training examples, especially with longer sequences, due to memory limitations affecting batching. Recent work has attempted to address these limitations.',\n",
       " 'The text discusses improvements in computational efficiency achieved through factorization tricks and conditional computation, with the latter also enhancing model performance. Despite these advancements, the constraint of sequential computation persists.',\n",
       " 'Attention mechanisms are crucial in sequence modeling and transduction tasks, enabling the modeling of dependencies irrespective of their distance in input or output sequences. Typically, these mechanisms are used alongside recurrent networks, with few exceptions.',\n",
       " 'In this work, the authors introduce the Transformer model, which replaces recurrence with an attention mechanism to capture global dependencies between input and output. This design enables greater parallelization, allowing the model to achieve state-of-the-art translation quality after just twelve hours of training on eight P100 GPUs.',\n",
       " 'The goal of reducing sequential computation underpins the design of the Extended Neural GPU, ByteNet, and ConvS2S, which utilize convolutional neural networks to compute hidden representations in parallel for all input and output positions. In these models, the operations needed to connect signals from two arbitrary positions increase with their distance, growing linearly in ConvS2S and logarithmically in ByteNet.',\n",
       " 'In the Transformer model, learning dependencies between distant positions is simplified to a constant number of operations, although it reduces the effective resolution because of averaging attention-weighted positions. This issue is addressed using Multi-Head Attention.',\n",
       " 'Self-attention, or intra-attention, is a mechanism that connects different positions within a single sequence to create a representation of that sequence. It has been effectively applied to tasks such as reading comprehension, abstractive summarization, textual entailment, and learning task-independent sentence representations.',\n",
       " 'End-to-end memory networks utilize a recurrent attention mechanism rather than sequence-aligned recurrence and have demonstrated strong performance in simple-language question answering and language modeling tasks.',\n",
       " 'The Transformer is the first transduction model that exclusively uses self-attention to compute input and output representations, eliminating the need for sequence-aligned RNNs or convolution. The subsequent sections will explain the Transformer, highlight the benefits of self-attention, and compare it to other models.',\n",
       " 'Most competitive neural sequence transduction models use an encoder-decoder architecture. The encoder transforms an input sequence of symbols into continuous representations, while the decoder generates an output sequence of symbols, one element at a time. The process is auto-regressive, meaning the decoder uses previously generated symbols as input for generating the next symbol.',\n",
       " 'The text describes the architecture of the Transformer model, which includes components such as multi-head attention, masked multi-head attention, feed forward networks, and positional encoding. The architecture features layers with operations like adding and normalizing, and involves both input and output embeddings, with outputs being shifted right.',\n",
       " 'The Transformer architecture consists of encoder and decoder stacks that utilize layered self-attention mechanisms and point-wise, fully connected layers. The encoder and decoder structures are depicted on the left and right sides of Figure 1.',\n",
       " 'The encoder consists of a stack of six identical layers, each containing two sub-layers: a multi-head self-attention mechanism and a position-wise fully connected feed-forward network. Residual connections and layer normalization are applied around each sub-layer, with the output being LayerNorm(x + Sublayer(x)).',\n",
       " 'All sub-layers and embedding layers in the model produce outputs with a dimension of 512, utilizing residual connections.',\n",
       " \"The decoder consists of a stack of six identical layers, each containing three sub-layers: two similar to those in the encoder and an additional one for multi-head attention over the encoder's output. It utilizes residual connections and layer normalization around each sub-layer. The self-attention sub-layer in the decoder is modified to prevent positions from attending to subsequent positions.\",\n",
       " 'Masking and offsetting the output embeddings by one position ensure that predictions for position i rely only on known outputs from earlier positions.',\n",
       " 'Attention functions map a query and a set of key-value pairs to an output, with all elements represented as vectors. The output is calculated as a weighted sum. The text introduces two types of attention mechanisms: Scaled Dot-Product Attention and Multi-Head Attention, the latter comprising multiple attention layers operating concurrently.',\n",
       " 'The text describes a process called \"Scaled Dot-Product Attention,\" which involves computing dot products between queries and keys, applying a softmax function to these products, and using the results to weigh the corresponding values. The dimensions of the queries and keys are denoted as \\\\(d_k\\\\), and the values as \\\\(d_v\\\\). The dot products are scaled by dividing by the square root of \\\\(d_k\\\\).',\n",
       " 'The attention function is calculated for a set of queries, keys, and values organized into matrices Q, K, and V, respectively. The outputs are determined using the formula: Attention(Q, K, V) = softmax((QK^T)/√dk)V.',\n",
       " 'The two primary attention functions are additive attention and dot-product (multiplicative) attention. Dot-product attention is similar to the presented algorithm but includes a scaling factor. Additive attention uses a feed-forward network with one hidden layer to compute the compatibility function. Although both have similar theoretical complexity, dot-product attention is faster and more space-efficient in practice due to its implementation advantages.',\n",
       " 'The text provides a summary of code optimized for matrix multiplication.',\n",
       " 'For small values of \\\\(d_k\\\\), additive and dot product attention perform similarly. However, for larger \\\\(d_k\\\\) values, additive attention is superior because dot product attention results in large magnitude dot products, which push the softmax function into areas with very small gradients. To address this, dot products are scaled by \\\\(1/\\\\sqrt{d_k}\\\\).',\n",
       " \"Multi-Head Attention involves projecting queries, keys, and values multiple times using learned linear projections to different dimensions. This allows the attention function to be performed in parallel on each set of projections, enhancing the model's ability to capture diverse patterns and relationships within the data.\",\n",
       " 'The dot product of vectors \\\\( q \\\\) and \\\\( k \\\\), whose components are independent random variables with mean 0 and variance 1, has a mean of 0 and a variance of \\\\( d_x \\\\). The output values are concatenated and projected to produce the final values, as shown in Figure 2.',\n",
       " 'Multi-head attention enables the model to focus on information from various representation subspaces simultaneously, overcoming the limitations of single-head attention where averaging can be restrictive. It is implemented by concatenating multiple attention heads, each with its own set of parameter matrices for queries, keys, and values, allowing for more nuanced and comprehensive attention mechanisms.',\n",
       " 'In this work, eight parallel attention layers (heads) are used, each with dimensions \\\\(d_k = d_v = \\\\frac{d_{model}}{h} = 64\\\\). This configuration maintains a similar computational cost to single-head attention with full dimensionality due to the reduced dimension of each head.',\n",
       " 'The Transformer model employs multi-head attention in three distinct ways. One of these is the \"encoder-decoder attention\" layers, where queries originate from the previous decoder layer, and the memory keys and values are derived from the encoder\\'s output. This setup allows each decoder position to focus on all positions in the input sequence, resembling traditional encoder-decoder attention mechanisms used in sequence-to-sequence models.',\n",
       " \"The encoder's self-attention layers allow each position to consider all positions from the previous layer by using keys, values, and queries derived from the same source, which is the output of the preceding encoder layer.\",\n",
       " 'Self-attention layers in the decoder enable each position to focus on all preceding positions, maintaining the auto-regressive property by preventing leftward information flow. This is achieved by masking out invalid connections in the scaled dot-product attention, setting them to -∞ before the softmax operation.',\n",
       " 'Each layer in both the encoder and decoder of the model includes a position-wise feed-forward network, which is applied independently to each position. This network consists of two linear transformations separated by a ReLU activation function.',\n",
       " 'The text describes a process where linear transformations, consistent across different positions, utilize distinct parameters at each layer. This is likened to two convolutions with a kernel size of 1. The input and output dimensions are both 512 (dmodel), while the inner-layer has a dimension of 2048 (dff). The section titled \"3.4 Embeddings and Softmax\" is mentioned, but not elaborated upon in the provided text.',\n",
       " 'The model uses learned embeddings to convert input and output tokens into vectors of dimension \\\\(d_{model}\\\\), and employs a learned linear transformation and softmax function to predict next-token probabilities from the decoder output. The weight matrix is shared between the embedding layers and the pre-softmax linear transformation, with weights in the embedding layers multiplied by \\\\(\\\\sqrt{d_{model}}\\\\).',\n",
       " 'Table 1 provides a comparison of various layer types based on three criteria: maximum path lengths, per-layer complexity, and the minimum number of sequential operations. The factors considered in the comparison include sequence length (n), representation dimension (d), kernel size for convolutions (k), and neighborhood size in restricted self-attention (r).',\n",
       " 'The text provides a comparison of different neural network layer types in terms of their computational complexity, sequential processing ability, and maximum path length for operations. The types discussed include Self-Attention, Recurrent, Convolutional, and Restricted Self-Attention layers. The complexity for each type is expressed in terms of the input size \\\\( n \\\\), dimensionality \\\\( d \\\\), and in the case of convolutional layers, the kernel size \\\\( k \\\\). The text highlights that Self-Attention layers have a complexity of \\\\( O(n^2 \\\\cdot d) \\\\) but can process inputs in parallel (sequential complexity \\\\( O(1) \\\\)), whereas Recurrent layers have a complexity of \\\\( O(n \\\\cdot d^2) \\\\) and require sequential processing (\\\\( O(n) \\\\)). Convolutional layers have a complexity of \\\\( O(k \\\\cdot n \\\\cdot d^2) \\\\) with a logarithmic path length \\\\( O(\\\\log_k(n)) \\\\), and Restricted Self-Attention layers, with complexity \\\\( O(r \\\\cdot n \\\\cdot d) \\\\), balance between full Self-Attention and recurrent layers, allowing limited parallel processing (\\\\( O(n/r) \\\\) maximum path length). The section on Positional Encoding is not detailed in the provided text.',\n",
       " 'The model lacks recurrence and convolution, so to utilize the sequence order, it incorporates \"positional encodings\" into the input embeddings at the base of the encoder and decoder stacks. These encodings share the same dimension as the embeddings, allowing them to be summed. Various options for positional encodings are available.',\n",
       " 'The text appears to be incomplete, providing only \"learned and fixed [9].\" Without additional context or information, it\\'s not possible to provide a meaningful summary.',\n",
       " 'This work utilizes sine and cosine functions at varying frequencies to model positional encoding, with specific formulas for even and odd indices.',\n",
       " 'The positional encoding assigns a sinusoidal pattern to each dimension, with wavelengths in a geometric progression from 2π to 10000 · 2π. This design is chosen to help the model learn relative positional relationships, as any fixed offset can be expressed as a linear function of the positional encoding.',\n",
       " 'The experiment compared learned positional embeddings with sinusoidal embeddings and found similar results. The sinusoidal version was chosen for its potential to extrapolate to longer sequence lengths beyond the training data.',\n",
       " 'In this section, the text compares self-attention layers with recurrent and convolutional layers, which are commonly used for mapping one variable-length sequence of symbol representations to another sequence of equal length. The discussion focuses on self-attention in the context of sequence transduction encoders or decoders. The motivation for using self-attention is based on three key desiderata.',\n",
       " 'Summary: The text discusses two aspects of computational complexity: the total complexity per layer and the degree of parallelization, measured by the minimum number of sequential operations needed.',\n",
       " 'The ability to learn long-range dependencies in sequence transduction tasks is influenced by the path length that forward and backward signals must traverse in a network. Shorter paths between input and output sequences make it easier to learn these dependencies.',\n",
       " 'Summary: The text discusses the maximum path length between any two input and output positions in networks composed of different layer types.',\n",
       " 'Self-attention layers are computationally more efficient than recurrent layers, as they connect all positions with a constant number of operations, while recurrent layers require O(n) sequential operations.',\n",
       " 'When sequence length \\\\( n \\\\) is smaller than the representation dimensionality \\\\( d \\\\), common in sentence representations used by advanced models like word-piece and byte-pair representations, computational performance for tasks with long sequences can be improved by restricting self-attention to a neighborhood of size \\\\( r \\\\) around each output position. This approach enhances efficiency.',\n",
       " 'The approach aims to reduce the path length to O(n/r), and further investigation is planned for future work.',\n",
       " 'A single convolutional layer with a kernel width smaller than the input size does not connect all input-output pairs, requiring multiple layers to achieve full connectivity. Contiguous kernels need O(n/k) layers, while dilated convolutions need O(logk(n)) layers, increasing path lengths between positions. Convolutional layers are generally more costly than recurrent layers by a factor of the kernel width. Separable convolutions, however, reduce this complexity.',\n",
       " 'The complexity of a separable convolution, even with k equal to n, matches the combined complexity of a self-attention layer and a point-wise feed-forward layer, which is the approach used in the model.',\n",
       " 'Self-attention in models can lead to more interpretable results, with individual attention heads learning to perform distinct tasks. These heads often reflect the syntactic and semantic structures of sentences, as discussed and exemplified in the appendix. The subsequent section outlines the training process for these models, including details on training data and batching.',\n",
       " 'We used the WMT 2014 English-German dataset with 4.5 million sentence pairs, encoded using byte-pair encoding with a shared vocabulary of about 37,000 tokens. For English-French, we used a larger WMT 2014 dataset with 36 million sentences and a 32,000 word-piece vocabulary. Sentence pairs were grouped by approximate sequence length for batching.',\n",
       " 'A dataset consisting of sentence pairs with roughly 25,000 tokens in both the source and target languages.',\n",
       " 'We trained our models using a machine with 8 NVIDIA P100 GPUs. The base models, using specified hyperparameters, required 0.4 seconds per training step and were trained for 100,000 steps over 12 hours. The big models, detailed in table 3, had a step time of 1.0 seconds and were trained for 300,000 steps over 3.5 days.',\n",
       " 'The text describes the use of the Adam optimizer with specific parameters (β1 = 0.9, β2 = 0.98, and ϵ = 10−9) and a learning rate schedule. The learning rate initially increases linearly for the first 4000 training steps (warmup_steps) and then decreases proportionally to the inverse square root of the step number.',\n",
       " 'The text discusses the use of three types of regularization during training and highlights the performance of the Transformer model. It notes that the Transformer achieves superior BLEU scores compared to previous state-of-the-art models for English-to-German and English-to-French translation tasks on the newstest2014 tests, while also being more cost-efficient in terms of training.',\n",
       " 'The text discusses two techniques used in model training: \\n\\n1. **Residual Dropout**: Dropout is applied to the output of each sub-layer before it is added to the input and normalized. It is also applied to the sums of embeddings and positional encodings in both encoder and decoder stacks with a dropout rate of 0.1 for the base model.\\n\\n2. **Label Smoothing**: This technique is used during training with a smoothing value of 0.1. While it negatively impacts perplexity by making the model less certain, it enhances accuracy and BLEU score.',\n",
       " 'The section titled \"6 Results\" includes a subsection \"6.1 Machine Translation,\" which likely discusses the findings or outcomes related to machine translation in a study or experiment. The summary would focus on the key results or insights derived from the machine translation component of the research. However, without additional details provided in the text, the specific results cannot be summarized further.',\n",
       " 'On the WMT 2014 English-to-German translation task, the big transformer model set a new state-of-the-art BLEU score of 28.4, outperforming previous models by more than 2.0 BLEU. This model was trained in 3.5 days using 8 P100 GPUs. Additionally, the base model also surpassed all previously published models and ensembles, with significantly lower training costs.',\n",
       " \"I'm sorry, but there is no detailed text provided for me to generate a summary. Could you please provide the text you'd like summarized?\",\n",
       " 'The big model achieved a BLEU score of 41.0 on the WMT 2014 English-to-French translation task, surpassing all previous single models, while incurring less than a quarter of the training cost of the prior state-of-the-art model. The Transformer (big) model used a dropout rate of 0.1 instead of 0.3.',\n",
       " 'The base models were created by averaging the last 5 checkpoints saved at 10-minute intervals, while the big models averaged the last 20 checkpoints. Beam search with a beam size of 4 and a length penalty of α = 0.6 was utilized, with hyperparameters chosen based on development set experiments. The maximum output length during inference was set to the input length plus 50, with early termination applied when feasible.',\n",
       " 'Table 2 presents a comparison of the translation quality and training costs of our model with other architectures from existing literature. The training costs are estimated by calculating the number of floating point operations, which is derived from the training time, the number of GPUs used, and the sustained single-precision floating-point capacity of each GPU.',\n",
       " 'The section \"6.2 Model Variations\" discusses the evaluation of different components of the Transformer model by altering the base model in various ways. The objective was to assess how these changes impact performance, specifically focusing on English-to-German translation tasks. Additionally, it mentions the use of specific TFLOPS values for different hardware (K80, K40, M40, and P100) in the experiments.',\n",
       " 'Table 3 presents different variations of the Transformer architecture, focusing on the English-to-German translation task using the newstest2013 development set. It highlights modifications from the base model, with all other unlisted parameters remaining consistent. The table provides per-wordpiece perplexity metrics based on byte-pair encoding, which are not directly comparable to per-word perplexities.',\n",
       " 'The text discusses an experiment involving variations in the number of attention heads and dimensions in a model, maintaining constant computational resources. Results on a development set (newstest2013) show that single-head attention performs 0.9 BLEU points worse than the best configuration, and quality decreases with an excessive number of heads. The outcomes are detailed in Table 3, using beam search without checkpoint averaging.',\n",
       " 'In Table 3, reducing the attention key size dk negatively impacts model quality, indicating that a more advanced compatibility function than dot product might be advantageous. Larger models perform better, and dropout effectively prevents overfitting. Replacing sinusoidal positional encoding with learned positional embeddings also shows promising results.',\n",
       " 'The summary is: The results are the same as those of the base model.',\n",
       " 'The text discusses experiments conducted to evaluate if the Transformer model can be applied to English constituency parsing. This task is challenging due to its strong structural constraints and the fact that the output is significantly longer than the input. Additionally, RNN sequence-to-sequence models have struggled to achieve state-of-the-art results in scenarios with limited data.',\n",
       " 'We trained a 4-layer transformer model with a model dimension of 1024 on the Wall Street Journal section of the Penn Treebank, consisting of about 40,000 training sentences. Additionally, we applied a semi-supervised approach using larger corpora, including high-confidence and BerkleyParser data, totaling approximately 17 million sentences. The vocabulary size was set to 16,000 tokens for the WSJ-only training and 32,000 tokens for the semi-supervised training.',\n",
       " 'The text describes a limited set of experiments conducted to select dropout rates, learning rates, and beam size for a model on the Section 22 development set, while keeping other parameters consistent with the English-to-German base translation model. The results, shown in Table 4, indicate that the Transformer model generalizes effectively to English constituency parsing, with the evaluation based on Section 23 of the WSJ.',\n",
       " 'The model demonstrated surprisingly strong performance, outperforming all previously reported models except the Recurrent Neural Network Grammar, despite not undergoing task-specific tuning. This was achieved by using an increased maximum output length of input length + 300, a beam size of 21, and α = 0.3 in both WSJ only and semi-supervised settings, as shown in Table 4.',\n",
       " 'The Transformer model, which relies entirely on attention mechanisms and replaces the recurrent layers in traditional encoder-decoder architectures with multi-headed self-attention, outperforms the Berkeley-Parser even when trained solely on the WSJ training set of 40K sentences.',\n",
       " 'The Transformer architecture allows for significantly faster training in translation tasks compared to recurrent or convolutional layer-based architectures. It achieves a new state-of-the-art performance on the WMT 2014 English-to-German and English-to-French translation tasks, with its best model surpassing all previously reported ensembles in the English-to-German task.',\n",
       " 'The future of attention-based models is promising, and there are plans to apply them to various tasks beyond text. This includes extending the Transformer to handle different input and output modalities like images, audio, and video, and exploring local, restricted attention mechanisms for efficient processing. Another research goal is to make generation less sequential.',\n",
       " 'The code for training and evaluating the models is accessible on GitHub at https://github.com/tensorflow/tensor2tensor. The authors express gratitude to Nal Kalchbrenner and Stephan Gouws for their valuable feedback and inspiration.',\n",
       " 'The text provides a list of references related to advancements in neural machine translation and normalization techniques in machine learning. The first reference discusses layer normalization, a method proposed by Ba, Kiros, and Hinton in 2016. The second reference is a 2014 paper by Bahdanau, Cho, and Bengio, which introduces a method for neural machine translation that involves learning to align and translate simultaneously. The third reference by Britz, Goldie, Luong, and Le (2017) explores various architectures for neural machine translation.',\n",
       " 'The provided text references two academic papers related to machine learning and natural language processing. The first paper, authored by Jianpeng Cheng, Li Dong, and Mirella Lapata in 2016, discusses the use of Long Short-Term Memory (LSTM) networks for machine reading. The second paper, authored by Kyunghyun Cho and colleagues in 2014, explores learning phrase representations using a recurrent neural network (RNN) encoder-decoder model for statistical machine translation.',\n",
       " \"The provided references are citations for academic papers related to deep learning and neural network models. Francois Chollet's paper discusses the Xception model, which utilizes depthwise separable convolutions to improve deep learning architectures. The work by Junyoung Chung and colleagues empirically evaluates gated recurrent neural networks (GRNNs) for sequence modeling tasks. Chris Dyer and co-authors explore the application of recurrent neural network grammars (RNNGs) in natural language processing, presented at the NAACL conference.\",\n",
       " 'The text lists three academic references related to neural network research. The first reference is a 2017 arXiv preprint by Jonas Gehring and colleagues on convolutional sequence to sequence learning. The second is a 2013 arXiv preprint by Alex Graves discussing generating sequences with recurrent neural networks. The third is a 2016 paper by Kaiming He and colleagues on deep residual learning for image recognition, presented at the IEEE Conference on Computer Vision and Pattern Recognition.',\n",
       " 'The provided references highlight significant contributions to the field of neural networks, particularly in the context of recurrent neural networks (RNNs) and long-term dependencies. The first reference discusses the challenges associated with gradient flow in recurrent nets, emphasizing the difficulty in learning long-term dependencies, a critical issue in training RNNs. The second reference introduces the Long Short-Term Memory (LSTM) model, developed by Sepp Hochreiter and Jürgen Schmidhuber, which addresses these challenges by providing a robust architecture capable of learning and retaining information over extended sequences, thereby improving the performance of RNNs in tasks involving long-term dependencies.',\n",
       " 'The provided text is a list of two citations from academic papers. The first citation refers to a paper by Zhongqiang Huang and Mary Harper presented at the 2009 Conference on Empirical Methods in Natural Language Processing, focusing on self-training PCFG grammars with latent annotations across languages. The second citation is a 2016 arXiv preprint by Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu, which explores the limits of language modeling.',\n",
       " 'The referenced works discuss advancements in neural network architectures. Kaiser and Bengio (2016) explore whether active memory can serve as a substitute for attention mechanisms in neural networks, while Kaiser and Sutskever (2016) demonstrate the ability of Neural GPUs to learn algorithms effectively. Kalchbrenner et al. (2017) present a method for neural machine translation that operates in linear time, offering a more efficient approach to processing translations.',\n",
       " 'The provided text is a list of references from academic papers, each focusing on different aspects of machine learning and neural networks. The first reference discusses structured attention networks as presented at the International Conference on Learning Representations in 2017. The second reference is about the Adam optimization method, introduced by Diederik Kingma and Jimmy Ba at ICLR in 2015. The third reference by Oleksii Kuchaiev and Boris Ginsburg covers factorization techniques for LSTM networks, documented in an arXiv preprint from 2017.',\n",
       " 'The text references two academic papers. The first, by Zhouhan Lin and colleagues, discusses a structured self-attentive mechanism for sentence embedding, published as an arXiv preprint in 2017. The second, by Minh-Thang Luong and collaborators, explores multi-task sequence to sequence learning, also available as an arXiv preprint, published in 2015.',\n",
       " 'The text references two academic papers. The first, by Minh-Thang Luong, Hieu Pham, and Christopher D. Manning, discusses effective methods for attention-based neural machine translation, published as an arXiv preprint in 2015. The second, by Mitchell P. Marcus, Mary Ann Marcinkiewicz, and Beatrice Santorini, details the construction of the Penn Treebank, a large annotated corpus of English, and was published in the journal \"Computational Linguistics\" in 1993.',\n",
       " 'The text references two academic papers. The first, authored by David McClosky, Eugene Charniak, and Mark Johnson, is titled \"Effective self-training for parsing\" and was presented at the Human Language Technology Conference of the NAACL in June 2006. The second paper, authored by Ankur Parikh, Oscar Täckström, Dipanjan Das, and Jakob Uszkoreit, is titled \"A decomposable attention model\" and was presented at the Empirical Methods in Natural Language Processing conference in 2016.',\n",
       " 'The text provides references to two academic papers. The first paper, authored by Romain Paulus, Caiming Xiong, and Richard Socher, is titled \"A deep reinforced model for abstractive summarization\" and was published as an arXiv preprint in 2017. The second paper, authored by Slav Petrov, Leon Barrett, Romain Thibaux, and Dan Klein, is titled \"Learning accurate, compact, and interpretable tree annotation\" and was presented at the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL in July 2006.',\n",
       " 'The text references two academic papers. The first paper by Ofir Press and Lior Wolf, published in 2016, discusses enhancing language models by utilizing the output embedding. The second paper by Rico Sennrich, Barry Haddow, and Alexandra Birch, published in 2015, explores the use of subword units to improve neural machine translation, particularly for handling rare words.',\n",
       " 'The text references two academic publications related to advancements in neural network architectures. The first is a 2017 paper by Noam Shazeer and colleagues on \"Outrageously large neural networks\" utilizing a sparsely-gated mixture-of-experts layer, which aims to improve the efficiency and scalability of neural networks. The second is a 2014 paper by Nitish Srivastava and collaborators introducing \"Dropout,\" a technique to prevent overfitting in neural networks by randomly dropping units during training, thereby enhancing model performance and generalization.',\n",
       " 'The provided text references two significant papers in the field of neural networks and machine learning. The first paper by Sainbayar Sukhbaatar and colleagues, published in 2015, discusses end-to-end memory networks, which likely focus on enhancing neural network models with memory components for improved information processing. The second paper by Ilya Sutskever, Oriol Vinyals, and Quoc VV Le, published in 2014, introduces sequence to sequence learning with neural networks, a foundational framework for various tasks involving sequential data, such as language translation. Both works are published in the proceedings of the Advances in Neural Information Processing Systems conference, highlighting their contributions to advancements in neural information processing.',\n",
       " 'The referenced texts are academic papers that contribute to the field of computer vision and natural language processing. The first paper, authored by Christian Szegedy and colleagues, discusses improvements and re-evaluations of the inception architecture, which is a type of deep learning model used for computer vision tasks. The second paper, by Vinyals, Kaiser, Koo, Petrov, Sutskever, and Hinton, presents a novel approach to grammar processing by treating it as a foreign language, as discussed in the context of the Neural Information Processing Systems conference in 2015.',\n",
       " \"The texts referenced are academic papers focusing on advancements in neural machine translation (NMT). The first paper by Yonghui Wu et al. discusses Google's neural machine translation system, highlighting its efforts to close the gap between human and machine translation capabilities. The second paper by Jie Zhou et al. explores the use of deep recurrent models with fast-forward connections to enhance the efficiency and performance of neural machine translation. Both papers contribute to the development of more accurate and effective machine translation technologies.\",\n",
       " 'The paper by Muhua Zhu, Yue Zhang, Wenliang Chen, Min Zhang, and Jingbo Zhu, presented at the 51st Annual Meeting of the ACL in August 2013, discusses a method for fast and accurate shift-reduce constituent parsing. The authors focus on improving the efficiency and precision of parsing algorithms, which are essential for syntactic analysis in natural language processing.',\n",
       " 'The text provided appears to be a nonsensical string of characters and symbols, making it difficult to extract meaningful information or summarize it effectively. It seems to lack coherent content or context for a summary.',\n",
       " 'Figure 3 illustrates how the attention mechanism in the encoder\\'s self-attention, specifically in layer 5 of 6, handles long-distance dependencies. The example shows multiple attention heads focusing on the distant dependency related to the verb \"making,\" which helps complete the phrase \"making...more difficult.\" The visualization highlights the attention given to the word \"making,\" with different colors representing different heads, and is best viewed in color.',\n",
       " 'The text appears to be a repeated sequence of letters and words in a non-coherent manner, possibly a placeholder or a formatting error, and does not provide meaningful content to summarize.',\n",
       " \"The text describes Figure 4, which focuses on two attention heads in a neural network model, specifically layer 5 of 6, that are involved in anaphora resolution. The top part of the figure shows the full attentions for head 5, while the bottom part highlights isolated attentions from the word 'its' for attention heads 5 and 6, noting that these attentions are particularly sharp for this word.\",\n",
       " \"The text describes a figure illustrating the behavior of attention heads within an encoder's self-attention mechanism at layer 5 of 6. It highlights that different attention heads have learned to perform distinct tasks, demonstrating an understanding of sentence structure.\"]"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_summaries=[summarize_text(i) for i in text_elements]\n",
    "text_summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "8d003d1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_table(table_element):\n",
    "    prompt = f\"Summarize the following table:\\n\\n{table_element}\\n\\nSummary:\"\n",
    "    response = chain_gpt.invoke([HumanMessage(content=prompt)])\n",
    "    return response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "212bf0c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The table compares several machine translation models based on their performance in translating from English to German (EN-DE) and English to French (EN-FR), measured by BLEU scores, along with their training costs in FLOPs. The ByteNet model has a BLEU score of 23.75 for EN-DE. The Deep-Att + PosUnk model achieves a BLEU score of 39.2 for EN-FR. The GNMT + RL model records 24.6 for EN-DE and 39.92 for EN-FR, with training costs of 2.3x10^19 and 1.4x10^70 FLOPs, respectively. The ConvS2S model has scores of 25.16 for EN-DE and 40.46 for EN-FR, with costs of 9.6x10^19 and 1.5x10^70 FLOPs. The MoE model scores 26.03 for EN-DE and 40.56 for EN-FR, with costs of 2.0x10^19 and 1.2x10^79 FLOPs. Ensemble versions of the Deep-Att + PosUnk and GNMT + RL models improve EN-FR scores to 40.4 and 41.16, respectively, with varying costs. The ConvS2S Ensemble scores 26.36 for EN-DE and 41.29 for EN-FR, with costs of 7.7x10^19 and 1.2x10^71 FLOPs. The Transformer models achieve the highest BLEU scores, with the base model scoring 27.3 for EN-DE and 38.1 for EN-FR, and the big model scoring 28.4 for EN-DE and 41.8 for EN-FR, with respective costs of 3.3x10^18 and 2.3x10^19 FLOPs.',\n",
       " 'The table contains a series of experiments or configurations labeled with different parameters and outcomes. Key elements include various parameter settings and their corresponding results. The experiments seem to be organized into different groups or categories, labeled (A), (B), (C), and (E), among others, with each configuration affecting the outcomes in the last two columns. Parameters like \"N,\" \"dyoast,\" \"de,\" and others vary, with corresponding results such as numerical values in columns that could represent performance metrics or other outcomes. The summary highlights that different configurations yield varying outcomes, with specific mention of positional embedding instead of sinusoids in group (E), resulting in measurements of 4.92 and 25.7.',\n",
       " 'The table presents a dataset with two main rows. The first row, labeled \"big,\" includes six numerical values: 1024, 4096, 16, 0.3, 300K. The second row contains three numerical values: 4.33, 26.4, 213. The context or specific meaning of these numbers is not provided.',\n",
       " 'The table compares the performance of various parsers on the WSJ (Wall Street Journal) dataset, evaluated using the F1 score. The parsers are categorized based on their training approach: WSJ only (discriminative) and semi-supervised.\\n\\nFor WSJ only (discriminative) training:\\n- Vinyals & Kaiser et al. (2014) achieved an F1 score of 88.3.\\n- Petrov et al. (2006) and Zhu et al. (2013) both achieved an F1 score of 90.4.\\n- Dyer et al. (2016) reached an F1 score of 91.7.\\n- A Transformer model with 4 layers achieved an F1 score of 91.3.\\n\\nFor semi-supervised training:\\n- Zhu et al. (2013), Huang & Harper (2009), and Vinyals & Kaiser et al. (2014) all achieved an F1 score of 91.3.\\n- McClosky et al. (2006) and Vinyals & Kaiser et al. (2014) reached the highest F1 score of 92.1 in this category.',\n",
       " 'The table presents the performance of different models and approaches in a specific task. A 4-layer Transformer model using a semi-supervised approach achieved a score of 92.7. An approach by Luong et al. (2015) using multi-task learning achieved a score of 93.0. Lastly, a generative approach by Dyer et al. (2016) achieved the highest score of 93.3.']"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table_summaries=[summarize_table(i) for i in table_elements]\n",
    "table_summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "a0c4ca3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def summarize_image(encoded_image):\n",
    "    prompt = [\n",
    "        AIMessage(content=\"You are a bot that is good at analyzing images.\"),\n",
    "        HumanMessage(content=[\n",
    "            {\"type\": \"text\", \"text\": \"Describe the contents of this image.\"},\n",
    "            {\n",
    "                \"type\": \"image_url\",\n",
    "                \"image_url\": {\n",
    "                    \"url\": f\"data:image/jpeg;base64,{encoded_image}\"\n",
    "                },\n",
    "            },\n",
    "        ])\n",
    "    ]\n",
    "    response = chain_gpt.invoke(prompt)\n",
    "    return response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "49913940",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_summaries=[summarize_image(i) for i in image_elements]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "9a2b3b71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The image appears to be a diagram or visualization showing a sequence of words, likely representing a sentence. The words are connected with lines, possibly indicating relationships or dependencies between them. The sentence seems to be about American governments and voting laws, mentioning the difficulty of the registration or voting process since 2009. There are elements like \"<EOS>\" and \"<pad>\" which suggest this could be related to a machine learning model\\'s processing of text data.',\n",
       " 'This image appears to depict a visual representation of text alignment or attention mapping. It shows two sentences with words connected by lines of varying thickness and opacity, indicating associations or correspondences between the words. The sentences are:\\n\\n1. \"The Law will never be perfect, but its application should be just.\"\\n2. \"This is what we are missing, in my opinion.\"\\n\\nThe connections suggest relationships between specific words in each sentence. Additionally, there are special tokens like `<EOS>` and `<pad>` at the end, which are often used in natural language processing tasks.',\n",
       " 'The image is a visualization of word alignment between two sentences or sequences, possibly showing a translation or transformation. Each word from the left sequence is connected to one or more words in the right sequence with lines of varying thickness. The thicker lines indicate stronger connections. The sequences include phrases like \"The Law will never be perfect,\" \"its application should be just,\" and \"this is what we are missing, in my opinion.\" The image also includes tags like `<EOS>` and `<pad>`, which are typically used in natural language processing to denote the end of a sequence and padding, respectively.',\n",
       " 'The image shows a diagram with two parallel columns of text, linked by lines of varying thickness. Each column contains a sequence of words or tokens, including special tokens like `<EOS>` and `<pad>`. The lines connecting the words indicate relationships or alignments between them, with thicker lines suggesting stronger connections. This type of diagram is typically used to visualize attention mechanisms in neural networks, showing how different parts of input and output sequences are related.',\n",
       " \"The image is a diagram of the Transformer model architecture used in machine learning, particularly for natural language processing tasks. It consists of two main parts: the encoder (on the left) and the decoder (on the right).\\n\\n### Encoder:\\n- **Input Embedding**: Converts input tokens into vectors.\\n- **Positional Encoding**: Adds information about the position of tokens to the embeddings.\\n- **Nx Repeated Layers**: \\n  - Each layer includes:\\n    - **Multi-Head Attention**: Mechanism that allows the model to focus on different parts of the input sequence simultaneously.\\n    - **Add & Norm**: Residual connection followed by layer normalization.\\n    - **Feed Forward**: A fully connected neural network applied to each position.\\n\\n### Decoder:\\n- **Output Embedding**: Converts output tokens (shifted right) into vectors.\\n- **Positional Encoding**: Similar to the encoder, adds positional information.\\n- **Nx Repeated Layers**:\\n  - Each layer includes:\\n    - **Masked Multi-Head Attention**: Ensures that positions do not attend to subsequent positions, maintaining the autoregressive property.\\n    - **Multi-Head Attention**: Attends to the encoder's output.\\n    - **Add & Norm**: Residual connection followed by layer normalization.\\n    - **Feed Forward**: A fully connected neural network applied to each position.\\n\\n### Final Layers:\\n- **Linear Layer**: Projects decoder outputs to the vocabulary size.\\n- **Softmax**: Converts the logits to probabilities over the vocabulary.\\n\\nThe model outputs probabilities for predicting the next token in a sequence.\",\n",
       " \"This image is a flowchart representing the process of scaled dot-product attention, commonly used in transformer models. Here's a breakdown of the steps:\\n\\n1. **MatMul**: The process starts with a matrix multiplication of `Q` (Query) and `K` (Key) matrices.\\n\\n2. **Scale**: The result is then scaled, usually by the square root of the dimension of the key vectors.\\n\\n3. **Mask (opt.)**: An optional masking step is applied. This is often used in decoder models to prevent attending to subsequent positions.\\n\\n4. **SoftMax**: The scaled (and optionally masked) scores are passed through a SoftMax function to obtain attention weights.\\n\\n5. **MatMul**: Finally, another matrix multiplication is performed with the attention weights and `V` (Value) matrix to produce the output.\\n\\nThe process involves the inputs `Q`, `K`, and `V` and results in a weighted combination of the values based on the attention scores.\",\n",
       " \"The image is a diagram illustrating the architecture of a Scaled Dot-Product Attention module, commonly used in transformers. Here's a breakdown of the components:\\n\\n1. **Inputs:**\\n   - \\\\(Q\\\\) (Query)\\n   - \\\\(K\\\\) (Key)\\n   - \\\\(V\\\\) (Value)\\n\\n2. **Linear Layers:**\\n   - Each of the inputs \\\\(Q\\\\), \\\\(K\\\\), and \\\\(V\\\\) goes through a separate linear transformation.\\n\\n3. **Scaled Dot-Product Attention:**\\n   - These transformed inputs are then fed into a Scaled Dot-Product Attention mechanism, which combines the queries, keys, and values.\\n\\n4. **Multiple Heads:**\\n   - The diagram shows multiple instances (or heads) of the attention mechanism, indicating a multi-head attention setup.\\n\\n5. **Concatenation:**\\n   - The outputs of these attention heads are concatenated.\\n\\n6. **Final Linear Layer:**\\n   - The concatenated output is passed through another linear layer to produce the final output.\\n\\nThis diagram is part of the attention mechanism in transformer models, showing how input sequences are processed to capture different relationships and dependencies.\"]"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "4591560d",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore = Chroma(collection_name=\"summaris\", embedding_function=OpenAIEmbeddings())\n",
    "store = InMemoryStore()\n",
    "id_key = \"doc_id\"\n",
    "retriever = MultiVectorRetriever(vectorstore=vectorstore, docstore=store, id_key=id_key,top_k=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "f1ca5099",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text Summary Exist\n",
      "Table Summary Exist\n",
      "Image Summary Exist\n"
     ]
    }
   ],
   "source": [
    "def add_documents_to_retriever(summaries, original_contents):\n",
    "    doc_ids = [str(uuid.uuid4()) for _ in summaries]\n",
    "    summary_docs = [\n",
    "        Document(page_content=s, metadata={id_key: doc_ids[i]})\n",
    "        for i, s in enumerate(summaries)\n",
    "    ]\n",
    "    retriever.vectorstore.add_documents(summary_docs)\n",
    "    retriever.docstore.mset(list(zip(doc_ids, original_contents)))\n",
    "\n",
    "\n",
    "# Add text summaries\n",
    "\n",
    "\n",
    "if len(text_summaries)>0:\n",
    "    print('Text Summary Exist')\n",
    "    add_documents_to_retriever(text_elements, text_elements)\n",
    "else :\n",
    "    print('Text is empty')\n",
    "\n",
    "# Add table summaries\n",
    "if len(table_summaries)>0:\n",
    "    print('Table Summary Exist')\n",
    "    add_documents_to_retriever(table_elements, table_elements)\n",
    "else :\n",
    "    print('Table is empty')\n",
    "\n",
    "# Add image summaries\n",
    "\n",
    "if len(image_summaries)>0:\n",
    "    print('Image Summary Exist')\n",
    "    add_documents_to_retriever(image_summaries, image_elements) \n",
    "else :\n",
    "    print('Image is empty')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "e0f823a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# template = \"\"\"Extract the following personal details based only on the context provided below. \n",
    "# The context may include text, images, and tables.\n",
    "\n",
    "# {context}\n",
    "\n",
    "# Return the result in this exact format:\n",
    "\n",
    "# Name                   : <Actual Name>\n",
    "# Email                  : <Actual Email>\n",
    "# Mobile Number          : <Actual Mobile Number>\n",
    "# Latest Education       : <Actual Latest Education>\n",
    "# KEY SKILL SET          : <Actual KEY SKILL SET >\n",
    "\n",
    "# Question: {question}\n",
    "# \"\"\"\n",
    "\n",
    "template = \"\"\"Extract the requested details.Do not give ambiguous answer.\n",
    "Answers should be to the point.\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "model = ChatOpenAI(temperature=0, model=\"gpt-4o\")\n",
    "\n",
    "chain = (\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "8d1a56e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SoftMax and MatMul are operations typically used in the decoder section of neural network architectures, such as transformers. \n",
      "\n",
      "- **SoftMax**: This is a function that converts a vector of raw scores (logits) into probabilities. It is often used in the final layer of a neural network to produce a probability distribution over possible output classes.\n",
      "\n",
      "- **MatMul**: This stands for matrix multiplication. It is a fundamental operation in neural networks used to compute the dot product between matrices, which is essential for transforming input data through the network layers.\n"
     ]
    }
   ],
   "source": [
    "question= \"What is SoftMax and MatMul in Decoder section\"\n",
    "answer = chain.invoke(question)\n",
    "print(answer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ragenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
